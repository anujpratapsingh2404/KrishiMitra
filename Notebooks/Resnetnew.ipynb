{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ee0aad-e091-4d3e-88d7-fc463a5cbc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3050 Laptop GPU, compute capability 8.6\n",
      "Found 72800 validated image filenames belonging to 91 classes.\n",
      "Found 9100 validated image filenames belonging to 91 classes.\n",
      "Found 9100 validated image filenames belonging to 91 classes.\n",
      "Steps per epoch: 1137\n",
      "Validation steps: 142\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "import math\n",
    "import multiprocessing\n",
    "\n",
    "try:\n",
    "    import psutil\n",
    "    monitor_memory = True\n",
    "except ImportError:\n",
    "    monitor_memory = False\n",
    "    print(\"psutil not installed; memory monitoring disabled. Install with: pip install psutil\")\n",
    "\n",
    "\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "data_dir = 'D:\\Major Project\\Dataset\\Resized, Augmented\\Colored_Augmented'\n",
    "batch_size = 64\n",
    "target_size = (224, 224)\n",
    "image_paths = []\n",
    "labels = []\n",
    "classes = sorted(os.listdir(data_dir))\n",
    "num_classes = len(classes)\n",
    "\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    for img_name in os.listdir(class_dir):\n",
    "        if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            image_paths.append(img_path)\n",
    "            labels.append(class_name)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n",
    "\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    channel_shift_range=20,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "workers = min(multiprocessing.cpu_count(), 4)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='image_path',\n",
    "    y_col='label',\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    workers=workers,\n",
    "    use_multiprocessing=True,\n",
    "    max_queue_size=10\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    x_col='image_path',\n",
    "    y_col='label',\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    workers=workers,\n",
    "    use_multiprocessing=True,\n",
    "    max_queue_size=10\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col='image_path',\n",
    "    y_col='label',\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    workers=workers,\n",
    "    use_multiprocessing=True,\n",
    "    max_queue_size=10\n",
    ")\n",
    "\n",
    "def generator_to_tfdata(generator, cache=False):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: generator,\n",
    "        output_types=(tf.float32, tf.float32),\n",
    "        output_shapes=([None, 224, 224, 3], [None, num_classes])\n",
    "    )\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = generator_to_tfdata(train_generator, cache=False)\n",
    "val_dataset = generator_to_tfdata(val_generator, cache=False)\n",
    "test_dataset = generator_to_tfdata(test_generator, cache=False)\n",
    "\n",
    "steps_per_epoch = len(train_df) // batch_size\n",
    "validation_steps = len(val_df) // batch_size\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Validation steps: {validation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cebcf21-2233-4e94-ae0c-05facfa36872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Model (Base Layers Frozen):\n",
      "Total parameters: 23862235\n",
      "Trainable parameters: 274267\n",
      "Non-trainable parameters: 23587968\n",
      "Epoch 1/50\n",
      "1137/1137 [==============================] - 984s 854ms/step - loss: 1.0751 - accuracy: 0.6989 - val_loss: 0.4219 - val_accuracy: 0.8485\n",
      "Epoch 2/50\n",
      "1137/1137 [==============================] - 911s 802ms/step - loss: 0.5419 - accuracy: 0.8195 - val_loss: 0.3587 - val_accuracy: 0.8763\n",
      "Epoch 3/50\n",
      "1137/1137 [==============================] - 854s 752ms/step - loss: 0.4658 - accuracy: 0.8421 - val_loss: 0.3200 - val_accuracy: 0.8821\n",
      "Epoch 4/50\n",
      "1137/1137 [==============================] - 852s 750ms/step - loss: 0.4289 - accuracy: 0.8519 - val_loss: 0.3127 - val_accuracy: 0.8898\n",
      "Epoch 5/50\n",
      "1137/1137 [==============================] - 856s 753ms/step - loss: 0.4039 - accuracy: 0.8603 - val_loss: 0.2694 - val_accuracy: 0.9014\n",
      "Epoch 6/50\n",
      "1137/1137 [==============================] - 840s 739ms/step - loss: 0.3810 - accuracy: 0.8677 - val_loss: 0.2833 - val_accuracy: 0.8981\n",
      "Epoch 7/50\n",
      "1137/1137 [==============================] - 835s 735ms/step - loss: 0.3697 - accuracy: 0.8700 - val_loss: 0.2734 - val_accuracy: 0.9000\n",
      "Epoch 8/50\n",
      "1137/1137 [==============================] - 837s 737ms/step - loss: 0.3520 - accuracy: 0.8760 - val_loss: 0.2553 - val_accuracy: 0.9056\n",
      "Epoch 9/50\n",
      "1137/1137 [==============================] - 850s 748ms/step - loss: 0.3448 - accuracy: 0.8799 - val_loss: 0.2536 - val_accuracy: 0.9105\n",
      "Epoch 10/50\n",
      "1137/1137 [==============================] - 840s 739ms/step - loss: 0.3342 - accuracy: 0.8826 - val_loss: 0.2521 - val_accuracy: 0.9154\n",
      "Epoch 11/50\n",
      "1137/1137 [==============================] - 838s 737ms/step - loss: 0.3244 - accuracy: 0.8848 - val_loss: 0.2375 - val_accuracy: 0.9149\n",
      "Epoch 12/50\n",
      "1137/1137 [==============================] - 837s 737ms/step - loss: 0.3151 - accuracy: 0.8899 - val_loss: 0.2190 - val_accuracy: 0.9243\n",
      "Epoch 13/50\n",
      "1137/1137 [==============================] - 836s 735ms/step - loss: 0.3129 - accuracy: 0.8902 - val_loss: 0.2371 - val_accuracy: 0.9120\n",
      "Epoch 14/50\n",
      "1137/1137 [==============================] - 837s 736ms/step - loss: 0.3140 - accuracy: 0.8887 - val_loss: 0.2314 - val_accuracy: 0.9158\n",
      "Epoch 15/50\n",
      "1137/1137 [==============================] - 834s 734ms/step - loss: 0.2998 - accuracy: 0.8932 - val_loss: 0.2192 - val_accuracy: 0.9214\n",
      "Epoch 16/50\n",
      "1137/1137 [==============================] - 835s 735ms/step - loss: 0.3001 - accuracy: 0.8935 - val_loss: 0.2158 - val_accuracy: 0.9230\n",
      "Epoch 17/50\n",
      "1137/1137 [==============================] - 834s 734ms/step - loss: 0.2927 - accuracy: 0.8950 - val_loss: 0.2186 - val_accuracy: 0.9214\n"
     ]
    }
   ],
   "source": [
    "def build_model(num_classes):\n",
    "    base_model = ResNet50(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    predictions = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "    model = models.Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model, base_model\n",
    "\n",
    "model, base_model = build_model(num_classes)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Initial Model (Base Layers Frozen):\")\n",
    "print(f\"Total parameters: {model.count_params()}\")\n",
    "print(f\"Trainable parameters: {sum([w.shape.num_elements() for w in model.trainable_weights])}\")\n",
    "print(f\"Non-trainable parameters: {model.count_params() - sum([w.shape.num_elements() for w in model.trainable_weights])}\")\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_df['label']),\n",
    "    y=train_df['label']\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "csv_logger_initial = CSVLogger('D:/Major Project/resnet/training_history_initial_resnet50.csv', separator=',', append=False)\n",
    "early_stopping_initial = EarlyStopping(monitor='val_accuracy', patience=5, mode='max', restore_best_weights=True)\n",
    "model_checkpoint_initial = ModelCheckpoint('D:/Major Project/resnet/best_initial_resnet50.keras', monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "\n",
    "history_initial = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=50,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[early_stopping_initial, model_checkpoint_initial, csv_logger_initial],\n",
    "    verbose=1\n",
    ")\n",
    "model.save_weights('D:/Major Project/resnet/initial_weights_resnet50.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dcbdd1-a023-4d47-9fec-61ff96ebf97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      " 104/1137 [=>............................] - ETA: 12:18 - loss: 0.3340 - accuracy: 0.8858"
     ]
    }
   ],
   "source": [
    "# Fine-Tuning: Step 1 (Last 10 Layers)\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = True\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5, clipnorm=1.0),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "csv_logger_fine_1 = CSVLogger('D:/Major Project/resnet/training_history_fine_resnet50_1.csv', separator=',', append=False)\n",
    "early_stopping_fine = EarlyStopping(monitor='val_accuracy', patience=5, mode='max', restore_best_weights=True)\n",
    "model_checkpoint_fine_1 = ModelCheckpoint('D:/Major Project/resnet/best_fine_tuned_resnet50_1.keras', monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "\n",
    "def cosine_decay(epoch, initial_lr=1e-5):\n",
    "    epochs = 15\n",
    "    lr = initial_lr * (1 + math.cos(epoch * math.pi / epochs)) / 2\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(cosine_decay)\n",
    "\n",
    "history_fine_1 = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=15,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[early_stopping_fine, model_checkpoint_fine_1, csv_logger_fine_1, lr_scheduler],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb776a4-3d75-4bc9-ab4f-93b84ebf28a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tuning: Step 2 (Last 30 Layers)\n",
    "for layer in base_model.layers[-30:]:\n",
    "    layer.trainable = True\n",
    "model.compile(optimizer=Adam(learning_rate=1e-6, clipnorm=1.0),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "csv_logger_fine_2 = CSVLogger('D:/Major Project/resnet/training_history_fine_resnet50_2.csv', separator=',', append=False)\n",
    "model_checkpoint_fine_2 = ModelCheckpoint('D:/Major Project/resnet/best_fine_tuned_resnet50_2.keras', monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "\n",
    "history_fine_2 = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=15,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[early_stopping_fine, model_checkpoint_fine_2, csv_logger_fine_2, lr_scheduler],\n",
    "    verbose=1\n",
    ")\n",
    "model.save_weights('D:/Major Project/resnet/all_weights_resnet50.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d0115-a0da-4ead-a6c3-635b95e1db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset, steps=len(test_df) // batch_size)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c67bb6f-6488-4cb9-8485-715d3afdffd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
